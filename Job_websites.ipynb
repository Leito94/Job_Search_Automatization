{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import json\n",
    "import random\n",
    "import pandas as pd\n",
    "from selenium import webdriver as wb\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "What are you looking for? data\n",
      "Where are you searching? berlin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "searching on stepstone...\n"
     ]
    }
   ],
   "source": [
    "class JobFinder:\n",
    "    def __init__(self, search, place):\n",
    "        self.search = search\n",
    "        self.place = place\n",
    "        self.search_components = self.search.split()\n",
    "        # empty list for the job info\n",
    "        self.joblist = []\n",
    "        \n",
    "        # get the user agent list so that I dont get blocked\n",
    "        with open(\"user_agent_list.json\", \"r\") as f:\n",
    "            self.user_agent_list = json.load(f)\n",
    "        \n",
    "    def search_stepstone(self):\n",
    "        \n",
    "        print(\"searching on stepstone...\")\n",
    "        # modify stepstone url\n",
    "        stepstone_search = self.search.replace(\" \", \"%20\")\n",
    "        stepstone_place = self.place.replace(\" \", \"%20\")\n",
    "        \n",
    "        # steptstone url\n",
    "        stepstone_url = \"https://www.stepstone.de/5/ergebnisliste.html?stf=freeText&ns=1&companyid=0&sourceofthesearchfield=resultlistpage%3Ageneral&qs=[]&cityid=0&ke={}&ws={}&radius=30&suid=e4f10731-b7c4-4e30-a419-08dcd96f8eed&ob=date&of={}\"\n",
    "        # list for the pages\n",
    "        n = 25\n",
    "        m = n * 5\n",
    "        # how many pages do we want to scrape?\n",
    "        indeed_page_list = range(0, m, n) # scrape first 10 pages\n",
    "        \n",
    "        stepstone_page_list = range(0, m, n)\n",
    "\n",
    "        for page in stepstone_page_list:\n",
    "            \n",
    "            # get random user agent\n",
    "            headers = {\"User-Agent\" :random.choice(self.user_agent_list)}\n",
    "            try:\n",
    "                r = requests.get(stepstone_url.format(stepstone_search, stepstone_place, page), headers = headers)\n",
    "                soup = bs(r.content, \"lxml\")\n",
    "                \n",
    "                body = soup.find(\"div\", class_ = \"ResultsSectionContainer-gdhf14-0 gvBCse\")\n",
    "                divs = body.find_all(\"div\", class_ = \"sc-fzXfOu\")\n",
    "                \n",
    "                for div in divs:\n",
    "                    try:\n",
    "                        word_not_found = False\n",
    "                        title = div.find(\"div\", class_ = \"sc-fzXfOw\").text\n",
    "                        for word in self.search_components:\n",
    "                            if word.lower() not in title.lower():\n",
    "                                word_not_found = True\n",
    "                        if word_not_found == True:\n",
    "                            continue\n",
    "                        \n",
    "                    except:\n",
    "                        title = \"\"\n",
    "                    try:\n",
    "                        company = div.find(\"div\", {\"data-at\" : \"job-item-company-name\"}).text\n",
    "                    except:\n",
    "                        company = \"\"\n",
    "                    try:\n",
    "                        city = div.find(\"li\", {\"data-at\" : \"job-item-location\"}).text\n",
    "                    except:\n",
    "                        city = \"\"\n",
    "                    try: \n",
    "                        summary = div.find(\"a\", {\"data-offer-meta-text-snippet-link\" : \"true\"}).text\n",
    "                    except:\n",
    "                        summary = \"\"\n",
    "                    try:\n",
    "                        job_link = div.find(\"a\")\n",
    "                        link = \"https://www.stepstone.de\" + job_link.attrs[\"href\"]\n",
    "                    except:\n",
    "                        link = stepstone_url.format(stepstone_search, stepstone_place, page)\n",
    "                    try:\n",
    "                        time = div.find(\"time\").text\n",
    "                        try:\n",
    "                            number, unit = div.find(\"time\").text.split()[1:3]\n",
    "                            number = int(number)\n",
    "                            if unit == \"Stunde\" or unit == \"Stunden\":\n",
    "                                time = round(number/24, 2)\n",
    "                            elif unit == \"Tag\" or unit == \"Tagen\":\n",
    "                                time = number\n",
    "                            elif unit == \"Woche\" or unit == \"Wochen\":\n",
    "                                time = number * 7    \n",
    "                        except:\n",
    "                            pass\n",
    "                    except:\n",
    "                        time = None\n",
    "                    \n",
    "                    source = \"stepstone\"\n",
    "\n",
    "                    job = {\n",
    "                                \"title\" : title,\n",
    "                                \"company\" : company,\n",
    "                                \"city\" : city,\n",
    "                                \"time\" : time,\n",
    "                                \"summary\" : summary,\n",
    "                                \"source\" : source,\n",
    "                                \"link\" : link\n",
    "                            }\n",
    "\n",
    "                    self.joblist.append(job)\n",
    "                    \n",
    "            except:\n",
    "                print(\"no data found on page \" + str(int(page / n) + 1))\n",
    "            \n",
    "        try:\n",
    "            df = pd.DataFrame(self.joblist)\n",
    "            return df\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    def search_indeed(self):\n",
    "        \n",
    "        print(\"searching on indeed...\")\n",
    "        n = 10\n",
    "        m = n * 5\n",
    "        # how many pages do we want to scrape?\n",
    "        indeed_page_list = range(0, m, n) # scrape first 10 pages\n",
    "        \n",
    "        # modify indeed url\n",
    "        indeed_search = self.search.replace(\" \", \"+\")\n",
    "        indeed_place = self.place.replace(\" \", \"+\")\n",
    "        \n",
    "        # indeed url\n",
    "        indeed_url = \"https://de.indeed.com/jobs?q={}&l={}&sort=date&start={}\"\n",
    "\n",
    "        for page in indeed_page_list:\n",
    "\n",
    "            # get random user agent\n",
    "            headers = {\"User-Agent\" :random.choice(self.user_agent_list)}\n",
    "            try:\n",
    "                \n",
    "                r = requests.get(indeed_url.format(indeed_search, indeed_place, page), headers = headers)\n",
    "                # check if we can access the website\n",
    "\n",
    "                soup = bs(r.content, \"lxml\")\n",
    "                divs = soup.find_all(\"div\", class_ = \"jobsearch-SerpJobCard\")\n",
    "            \n",
    "\n",
    "                for div in divs:\n",
    "                    try:\n",
    "                        word_not_found = False\n",
    "                        title = div.find(\"a\").text.strip()\n",
    "                        for word in self.search_components:\n",
    "                            if word.lower() not in title.lower():\n",
    "                                word_not_found = True\n",
    "                        if word_not_found == True:\n",
    "                            continue\n",
    "                    except:\n",
    "                        title = \"\"\n",
    "                    try:\n",
    "                        company = div.find(\"span\", class_ = \"company\").text.strip()\n",
    "                    except:\n",
    "                        company = \"\"\n",
    "                    try:\n",
    "                        city = div.find(\"span\", class_ = \"location accessible-contrast-color-location\").text.strip()\n",
    "                    except:\n",
    "                        city = \"\"\n",
    "                    try:\n",
    "                        summary = div.find(\"div\", {\"class\" : \"summary\"}).text.strip().replace(\"\\n\",\"\")\n",
    "                    except:\n",
    "                        summary = \"\"\n",
    "                    try:\n",
    "                        job_link = div.find(\"h2\", {\"class\" : \"title\"}).find(\"a\")[\"href\"]\n",
    "                        link = \"https://www.indeed.com\" + job_link\n",
    "                    except:\n",
    "                        link = indeed_url.format(indeed_search, indeed_place, page)\n",
    "                    try:\n",
    "                        time = div.find(\"span\", {\"class\" : \"date\"}).text\n",
    "                        try:\n",
    "                            unit = time.split()[0]\n",
    "                            if unit == \"Gerade\":\n",
    "                                time = 0\n",
    "                            elif unit == \"Heute\":\n",
    "                                time = 0.5\n",
    "                            else:\n",
    "                                number = int(time.split()[1])\n",
    "                                time = number\n",
    "                        except:\n",
    "                            pass\n",
    "                           \n",
    "                    except:\n",
    "                        time = None\n",
    "                    source = \"indeed\"\n",
    "\n",
    "                    job = {\n",
    "                                \"title\" : title,\n",
    "                                \"company\" : company,\n",
    "                                \"city\" : city,\n",
    "                                \"time\" : time,\n",
    "                                \"summary\" : summary,\n",
    "                                \"source\" : source,\n",
    "                                \"link\" : link\n",
    "                            }\n",
    "\n",
    "                    self.joblist.append(job)\n",
    "            except:\n",
    "                print(\"no data found on page \" + str(int(page / n) + 1))\n",
    "            \n",
    "        try:\n",
    "            df = pd.DataFrame(self.joblist)\n",
    "            return df\n",
    "        except:\n",
    "            return None\n",
    "        \n",
    "    def search_stack_overflow(self):\n",
    "        \n",
    "        print(\"searching on stack overflow...\")\n",
    "        \n",
    "        # list for the pages\n",
    "        stack_overflow_page_list = range(1, 5)\n",
    "        # stack overflow url\n",
    "        stack_overflow_url = \"https://stackoverflow.com/jobs?d=20&l={}&q={}&u=Km&pg={}\"\n",
    "        \n",
    "        # modify stack_overflow url\n",
    "        stack_overflow_search = self.search.replace(\" \", \"+\")\n",
    "        stack_overflow_place = self.place.replace(\" \", \"+\")\n",
    "        \n",
    "        for page in stack_overflow_page_list:\n",
    "            # get random user agent\n",
    "            headers = {\"User-Agent\" :random.choice(self.user_agent_list)}\n",
    "            try:\n",
    "                r = requests.get(stack_overflow_url.format(stack_overflow_place, stack_overflow_search, page), headers = headers)\n",
    "                soup = bs(r.content, \"lxml\")\n",
    "\n",
    "                # list of the jobs\n",
    "                body = soup.find(\"div\", class_ = \"listResults\")\n",
    "                jobs = body.find_all(\"div\", {\"class\" : \"-job\"})\n",
    "           \n",
    "\n",
    "                for job in jobs:\n",
    "                    try:\n",
    "                        word_not_found = False\n",
    "                        title = job.find(\"a\", {\"class\" : \"s-link stretched-link\"}).text\n",
    "                        for word in self.search_components:\n",
    "                            if word.lower() not in title.lower():\n",
    "                                word_not_found = True\n",
    "                        if word_not_found == True:\n",
    "                            continue\n",
    "                    except:\n",
    "                        title = \"\"\n",
    "                    try:    \n",
    "                        company = job.find(\"h3\", {\"class\" : \"fc-black-700 fs-body1 mb4\"}).find(\"span\").text.strip()\n",
    "                    except:\n",
    "                        company = \"\"\n",
    "                    try:\n",
    "                        city = job.find(\"h3\", {\"class\" : \"fc-black-700 fs-body1 mb4\"}).find(\"span\", {\"class\" : \"fc-black-500\"}).text.strip()\n",
    "                    except:\n",
    "                        city = \"\"\n",
    "                    try:\n",
    "                        time = int(job.find(\"ul\", {\"class\" : \"mt4\"}).li.span.text.split(\"d\")[0])\n",
    "                    except:\n",
    "                        time = None\n",
    "                    summary = \"\"\n",
    "                    source = \"stack overflow\"\n",
    "\n",
    "                    job = {\n",
    "                                \"title\" : title,\n",
    "                                \"company\" : company,\n",
    "                                \"city\" : city,\n",
    "                                \"time\" : time,\n",
    "                                \"summary\" : summary,\n",
    "                                \"source\" : source,\n",
    "                                \"link\" : stack_overflow_url.format(stack_overflow_place, stack_overflow_search, page)\n",
    "                            }\n",
    "\n",
    "                    self.joblist.append(job)\n",
    "            \n",
    "            except:\n",
    "                print(\"no data found on page \" + str(page))\n",
    "        try:\n",
    "            df = pd.DataFrame(self.joblist)\n",
    "            return df\n",
    "        except:\n",
    "            return None\n",
    "        \n",
    "    def search_xing(self):\n",
    "        \n",
    "        print(\"searching on XING...\")\n",
    "        \n",
    "        xing_page_list = range(1, 5)\n",
    "\n",
    "        # xing url\n",
    "        xing_url = \"https://www.xing.com/jobs/search?page={}&utf8=%E2%9C%93&nrs=1&keywords={}&location={}&radius=&sort=date\"\n",
    "        chromedriver = r\"C:/Users/Leonhard/Downloads/chromedriver_win32/chromedriver.exe\"\n",
    "        \n",
    "        # for xing url\n",
    "        xing_search = self.search.replace(\" \", \"%20\")\n",
    "        xing_place = self.place.replace(\" \", \"%20\")\n",
    "        \n",
    "        # I dont want chrome to open\n",
    "        options = wb.ChromeOptions()\n",
    "        options.add_argument(\"headless\")\n",
    "        # add user agent\n",
    "        headers = random.choice(self.user_agent_list)\n",
    "        options.add_argument(f\"user-agent:{headers}\")\n",
    "        \n",
    "        for page in xing_page_list:\n",
    "            try:\n",
    "                # need to use selenium\n",
    "                webD = wb.Chrome(chromedriver, options=options)\n",
    "                webD.get(xing_url.format(page, xing_search, xing_place))\n",
    "                soup = bs(webD.page_source, \"lxml\")\n",
    "                body = soup.body\n",
    "                job_list = body.find(\"div\", {\"class\" : \"result-list-result-list-container-8d38ca5b\"})\n",
    "                jobs = job_list.find_all(\"div\", {\"class\" : \"result-result-container-6e907078\"})\n",
    "                \n",
    "                infos = [job.a for job in jobs]\n",
    "                for info in infos:\n",
    "                    try:\n",
    "                        word_not_found = False\n",
    "                        title = info.find(\"h2\").text.strip()\n",
    "                        for word in self.search_components:\n",
    "                            if word.lower() not in title.lower():\n",
    "                                word_not_found = True\n",
    "                        if word_not_found == True:\n",
    "                            continue\n",
    "                    except:\n",
    "                        title = \"\"\n",
    "                    try:\n",
    "                        company = info.find(\"div\", {\"class\" : \"result-result-subtitle-99125938\"}).text.strip().split(\",\")[0]\n",
    "                    except:\n",
    "                        company = \"\"\n",
    "                    try:\n",
    "                        city = info.find(\"div\", {\"class\" : \"result-result-subtitle-99125938\"}).text.strip().split(\",\")[1]\n",
    "                    except:\n",
    "                        city = \"\"\n",
    "                    try:\n",
    "                        time = info.time.text.strip()\n",
    "                        try:\n",
    "                            time_components = time.split()\n",
    "                            value = int(time_components[0])\n",
    "                            unit = time_components[1]\n",
    "                            if unit == \"minutes\" or unit == \"minute\":\n",
    "                                time = round(value / 1440, 2)\n",
    "                            elif unit == \"hour\" or unit == \"hours\":\n",
    "                                time = round(value / 24, 2)\n",
    "                            elif unit == \"day\" or unit == \"days\":\n",
    "                                time = value\n",
    "                        except:\n",
    "                            pass\n",
    "                    except:\n",
    "                        time = None\n",
    "                    try:\n",
    "                        summary = info.find(\"div\", {\"class\" : \"result-result-description-c7581001\"}).text.strip()\n",
    "                    except:\n",
    "                        summary = \"\"\n",
    "                        \n",
    "                    source = \"XING\"\n",
    "\n",
    "                    job = {\n",
    "                                \"title\" : title,\n",
    "                                \"company\" : company,\n",
    "                                \"city\" : city,\n",
    "                                \"time\" : time,\n",
    "                                \"summary\" : summary,\n",
    "                                \"source\" : source,\n",
    "                                \"link\" : xing_url.format(page, xing_search, xing_place)\n",
    "                            }\n",
    "                    self.joblist.append(job)\n",
    "            \n",
    "            except:\n",
    "                print(\"no data found on page \" + str(page))\n",
    "        try:\n",
    "            df = pd.DataFrame(self.joblist)\n",
    "            return df\n",
    "        except:\n",
    "            return None\n",
    "        \n",
    "    def search_linkedin(self):\n",
    "        \n",
    "        print(\"Searching on LinkedIn...\")\n",
    "        n = 25\n",
    "        m = n * 5\n",
    "        # how many pages do we want to scrape?\n",
    "        linkedin_page_list = range(0, m, n) # scrape first 5 pages\n",
    "        \n",
    "        linkedin_url = \"https://www.linkedin.com/jobs/search/?geoId=103035651&keywords={}&location={}&sortBy=DD&start={}\"\n",
    "        \n",
    "        # to modify url\n",
    "        linkedin_search = self.search.replace(\" \", \"%20\")\n",
    "        linkedin_place = self.place.replace(\" \", \"%20\")\n",
    "        \n",
    "        for page in linkedin_page_list:\n",
    "            try:\n",
    "                headers = {\"User-Agent\" : random.choice(self.user_agent_list)}\n",
    "                r = requests.get(linkedin_url.format(linkedin_search, linkedin_place, page), headers = headers)\n",
    "                soup = bs(r.content, \"lxml\")\n",
    "                job_list = soup.find(\"section\", {\"class\" : \"results__list\"}).find(\"ul\")\n",
    "\n",
    "                for job in job_list:\n",
    "                    try:\n",
    "                        word_not_found = False\n",
    "                        title = job.a.span.text.strip()\n",
    "                        for word in self.search_components:\n",
    "                            if word.lower() not in title.lower():\n",
    "                                word_not_found = True\n",
    "                        if word_not_found == True:\n",
    "                            continue\n",
    "                    except:\n",
    "                        title = \"\"\n",
    "                    try:\n",
    "                        company = job.find(\"h4\").text.strip()\n",
    "                    except:\n",
    "                        company = \"\"\n",
    "                    try:\n",
    "                        city = job.find(\"span\", {\"class\" : \"job-result-card__location\"}).text.strip()\n",
    "                    except:\n",
    "                        city = \"\"\n",
    "                    try:\n",
    "                        time = job.find(\"time\").text\n",
    "                        try:\n",
    "                            time_components = time.split()\n",
    "                            value = int(time_components[0])\n",
    "                            unit = time_components[1]\n",
    "                            if unit == \"minutes\" or unit == \"minute\":\n",
    "                                time = round(value / 1440, 2)\n",
    "                            elif unit == \"hour\" or unit == \"hours\":\n",
    "                                time = round(value / 24, 2)\n",
    "                            elif unit == \"day\" or unit == \"days\":\n",
    "                                time = value\n",
    "                        except:\n",
    "                            pass\n",
    "                    except:\n",
    "                        time = None\n",
    "                        \n",
    "                    summary = \"\"\n",
    "                    source = \"LinkedIn\"\n",
    "                 \n",
    "                    job = {\n",
    "                                \"title\" : title,\n",
    "                                \"company\" : company,\n",
    "                                \"city\" : city,\n",
    "                                \"time\" : time,\n",
    "                                \"summary\" : summary,\n",
    "                                \"source\" : source,\n",
    "                                \"link\" : linkedin_url.format(linkedin_search, linkedin_place, page)\n",
    "                            }\n",
    "\n",
    "                    self.joblist.append(job)\n",
    "                    \n",
    "            except:\n",
    "                print(\"no data found on page \" + str(int(page / n) + 1))\n",
    "        try:\n",
    "            df = pd.DataFrame(self.joblist)\n",
    "            return df\n",
    "        except:\n",
    "            return None\n",
    "                \n",
    "                \n",
    "                \n",
    "    def search_all(self):\n",
    "        \n",
    "        self.search_indeed()\n",
    "        self.search_stepstone()\n",
    "        self.search_stack_overflow()\n",
    "        self.search_xing()\n",
    "        self.search_linkedin()\n",
    "        \n",
    "        df = pd.DataFrame(self.joblist)\n",
    "        \n",
    "        try:\n",
    "            df = df.sort_values(by = \"time\")\n",
    "            df.reset_index(drop = True, inplace = True)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        print(\"Joblist is ready!\")\n",
    "\n",
    "        return df\n",
    "    \n",
    "    \n",
    "search = input(\"What are you looking for?\")\n",
    "place = input(\"Where are you searching?\")\n",
    "\n",
    "#search = \"Data Analyst\"\n",
    "#place = \"Berlin\"\n",
    "\n",
    "now = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "jobs = JobFinder(search, place)\n",
    "df = jobs.search_stepstone()\n",
    "\n",
    "# save as excel file\n",
    "#df.to_excel(f\"{now}_job_{search}_in_{place}.xlsx\", index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.stepstone.de/stellenangebote--Data-Scientist-w-m-d-Berlin-Rhenus-SN-digital-GmbH-Co-KG--6980135-inline.html'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.link[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
