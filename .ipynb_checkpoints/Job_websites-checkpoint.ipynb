{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import json\n",
    "import random\n",
    "import pandas as pd\n",
    "from selenium import webdriver as wb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "what are you looking for data scientist\n",
      "Where are you searching? deutschland\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "searching on indeed...\n",
      "searching on stepstone...\n",
      "searching on stack overflow...\n",
      "searching on XING...\n",
      "no data found on page 1\n",
      "no data found on page 2\n",
      "no data found on page 3\n",
      "no data found on page 4\n",
      "Searching on LinkedIn...\n",
      "0\n",
      "25\n",
      "25\n",
      "25\n",
      "50\n",
      "25\n",
      "75\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "search = input(\"What are you looking for\")\n",
    "place = input(\"Where are you searching?\")\n",
    "\n",
    "#search = \"Data Analyst\"\n",
    "#place = \"Berlin\"\n",
    "\n",
    "class JobFinder:\n",
    "    def __init__(self, search, place):\n",
    "        self.search = search\n",
    "        self.place = place\n",
    "        \n",
    "        # empty list for the job info\n",
    "        self.joblist = []\n",
    "        \n",
    "        # get the user agent list so that I dont get blocked\n",
    "        with open(\"../Data/user_agent_list.json\", \"r\") as f:\n",
    "            self.user_agent_list = json.load(f)\n",
    "        \n",
    "    def search_steptstone(self):\n",
    "        \n",
    "        print(\"searching on stepstone...\")\n",
    "        # modify stepstone url\n",
    "        stepstone_search = self.search.replace(\" \", \"%20\")\n",
    "        stepstone_place = self.place.replace(\" \", \"%20\")\n",
    "        \n",
    "        # steptstone url\n",
    "        stepstone_url = \"https://www.stepstone.de/5/ergebnisliste.html?stf=freeText&ns=1&companyid=0&sourceofthesearchfield=resultlistpage%3Ageneral&qs=[]&cityid=0&ke={}&ws={}&radius=30&suid=e4f10731-b7c4-4e30-a419-08dcd96f8eed&ob=date&of={}\"\n",
    "\n",
    "        # list for the pages\n",
    "        stepstone_page_list = range(0, 200, 25)\n",
    "\n",
    "        for page in stepstone_page_list:\n",
    "            \n",
    "            # get random user agent\n",
    "            headers = {\"User-Agent\" :random.choice(self.user_agent_list)}\n",
    "            try:\n",
    "                r = requests.get(stepstone_url.format(stepstone_search, stepstone_place, page), headers = headers)\n",
    "                soup = bs(r.content, \"lxml\")\n",
    "\n",
    "                body = soup.find(\"div\", class_ = \"ResultsSectionContainer-gdhf14-0 gvBCse\")\n",
    "                divs = body.find_all(\"div\", class_ = \"sc-fzXfOu\")\n",
    "                \n",
    "                for div in divs:\n",
    "                    try:\n",
    "                        title = div.find(\"div\", class_ = \"sc-fzXfOw\").text\n",
    "                    except:\n",
    "                        title = \"\"\n",
    "                    try:\n",
    "                        company = div.find(\"div\", {\"data-at\" : \"job-item-company-name\"}).text\n",
    "                    except:\n",
    "                        company = \"\"\n",
    "                    try:\n",
    "                        city = div.find(\"li\", {\"data-at\" : \"job-item-location\"}).text\n",
    "                    except:\n",
    "                        city = \"\"\n",
    "                    try: \n",
    "                        summary = div.find(\"a\", {\"data-offer-meta-text-snippet-link\" : \"true\"}).text\n",
    "                    except:\n",
    "                        summary = \"\"\n",
    "                    source = \"stepstone\"\n",
    "\n",
    "                    job = {\n",
    "                        \"title\" : title,\n",
    "                        \"company\" : company,\n",
    "                        \"city\" : city,\n",
    "                        \"summary\" : summary,\n",
    "                        \"source\" : source\n",
    "                    }\n",
    "\n",
    "                    self.joblist.append(job)\n",
    "                    \n",
    "            except:\n",
    "                print(\"no data found on page \" + str(page))\n",
    "            \n",
    "        try:\n",
    "            df = pd.DataFrame(self.joblist)\n",
    "            return df\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    def search_indeed(self):\n",
    "        \n",
    "        print(\"searching on indeed...\")\n",
    "        # how many pages do we want to scrape?\n",
    "        indeed_page_list = range(0, 100, 10) # scrape first 10 pages\n",
    "        \n",
    "        # modify indeed url\n",
    "        indeed_search = self.search.replace(\" \", \"+\")\n",
    "        indeed_place = self.place.replace(\" \", \"+\")\n",
    "        \n",
    "        # indeed url\n",
    "        indeed_url = \"https://de.indeed.com/jobs?q={}&l={}&sort=date&start={}\"\n",
    "\n",
    "        for page in indeed_page_list:\n",
    "\n",
    "            # get random user agent\n",
    "            headers = {\"User-Agent\" :random.choice(self.user_agent_list)}\n",
    "            try:\n",
    "                \n",
    "                r = requests.get(indeed_url.format(indeed_search, indeed_place, page), headers = headers)\n",
    "                # check if we can access the website\n",
    "\n",
    "                soup = bs(r.content, \"lxml\")\n",
    "                divs = soup.find_all(\"div\", class_ = \"jobsearch-SerpJobCard\")\n",
    "            \n",
    "\n",
    "                for div in divs:\n",
    "                    try:\n",
    "                        title = div.find(\"a\").text.strip()\n",
    "                    except:\n",
    "                        title = \"\"\n",
    "                    try:\n",
    "                        company = div.find(\"span\", class_ = \"company\").text.strip()\n",
    "                    except:\n",
    "                        company = \"\"\n",
    "                    try:\n",
    "                        city = div.find(\"span\", class_ = \"location accessible-contrast-color-location\").text.strip()\n",
    "                    except:\n",
    "                        city = \"\"\n",
    "                    try:\n",
    "                        summary = div.find(\"div\", {\"class\" : \"summary\"}).text.strip().replace(\"\\n\",\"\")\n",
    "                    except:\n",
    "                        summary = \"\"\n",
    "                    source = \"indeed\"\n",
    "\n",
    "                    job = {\n",
    "                        \"title\" : title,\n",
    "                        \"company\" : company,\n",
    "                        \"city\" : city,\n",
    "                        \"summary\" : summary,\n",
    "                        \"source\" : source\n",
    "                    }\n",
    "\n",
    "                    self.joblist.append(job)\n",
    "            except:\n",
    "                print(\"no data found on page \" + str(page))\n",
    "            \n",
    "        try:\n",
    "            df = pd.DataFrame(self.joblist)\n",
    "            return df\n",
    "        except:\n",
    "            return None\n",
    "        \n",
    "    def search_stack_overflow(self):\n",
    "        \n",
    "        print(\"searching on stack overflow...\")\n",
    "        \n",
    "        # list for the pages\n",
    "        stack_overflow_page_list = range(0, 5)\n",
    "        # stack overflow url\n",
    "        stack_overflow_url = \"https://stackoverflow.com/jobs?d=20&l={}&q={}&u=Km&pg={}\"\n",
    "        \n",
    "        # modify stack_overflow url\n",
    "        stack_overflow_search = self.search.replace(\" \", \"+\")\n",
    "        stack_overflow_place = self.place.replace(\" \", \"+\")\n",
    "        \n",
    "        for page in stack_overflow_page_list:\n",
    "            # get random user agent\n",
    "            headers = {\"User-Agent\" :random.choice(self.user_agent_list)}\n",
    "            try:\n",
    "                r = requests.get(stack_overflow_url.format(stack_overflow_place, stack_overflow_search, page), headers = headers)\n",
    "                soup = bs(r.content, \"lxml\")\n",
    "\n",
    "                # list of the jobs\n",
    "                body = soup.find(\"div\", class_ = \"listResults\")\n",
    "                jobs = body.find_all(\"div\", {\"class\" : \"-job\"})\n",
    "           \n",
    "\n",
    "                for job in jobs:\n",
    "                    try:\n",
    "                        title = job.find(\"a\", {\"class\" : \"s-link stretched-link\"}).text\n",
    "                    except:\n",
    "                        title = \"\"\n",
    "                    try:    \n",
    "                        company = job.find(\"h3\", {\"class\" : \"fc-black-700 fs-body1 mb4\"}).find(\"span\").text.strip()\n",
    "                    except:\n",
    "                        company = \"\"\n",
    "                    try:\n",
    "                        city = job.find(\"h3\", {\"class\" : \"fc-black-700 fs-body1 mb4\"}).find(\"span\", {\"class\" : \"fc-black-500\"}).text.strip()\n",
    "                    except:\n",
    "                        city = \"\"\n",
    "                    summary = \"\"\n",
    "                    source = \"stack overflow\"\n",
    "\n",
    "                    job = {\n",
    "                        \"title\" : title,\n",
    "                        \"company\" : company,\n",
    "                        \"city\" : city,\n",
    "                        \"summary\" : summary,\n",
    "                        \"source\" : source\n",
    "                    }\n",
    "\n",
    "                    self.joblist.append(job)\n",
    "            \n",
    "            except:\n",
    "                print(\"no data found on page \" + str(page))\n",
    "        try:\n",
    "            df = pd.DataFrame(self.joblist)\n",
    "            return df\n",
    "        except:\n",
    "            return None\n",
    "        \n",
    "    def search_xing(self):\n",
    "        \n",
    "        print(\"searching on XING...\")\n",
    "        \n",
    "        xing_page_list = range(1, 5)\n",
    "\n",
    "        # xing url\n",
    "        xing_url = \"https://www.xing.com/jobs/search?page={}&utf8=%E2%9C%93&nrs=1&keywords={}&location={}&radius=&sort=date\"\n",
    "        chromedriver = r\"C:/Users/Leonhard/Downloads/chromedriver_win32/chromedriver.exe\"\n",
    "        \n",
    "        # for xing url\n",
    "        xing_search = self.search.replace(\" \", \"%20\")\n",
    "        xing_place = self.place.replace(\" \", \"%20\")\n",
    "        \n",
    "        # I dont want chrome to open\n",
    "        options = wb.ChromeOptions()\n",
    "        options.add_argument(\"headless\")\n",
    "        # add user agent\n",
    "        headers = random.choice(self.user_agent_list)\n",
    "        options.add_argument(f\"user-agent={headers}\")\n",
    "        \n",
    "        for page in xing_page_list:\n",
    "            try:\n",
    "                # need to use selenium\n",
    "                webD = wb.Chrome(chromedriver, options=options)\n",
    "                webD.get(xing_url.format(page, xing_search, xing_place))\n",
    "                soup = bs(webD.page_source, \"lxml\")\n",
    "                body = soup.body\n",
    "                job_list = body.find(\"div\", {\"class\" : \"result-list-result-list-container-8d38ca5b\"})\n",
    "                jobs = job_list.find_all(\"div\", {\"class\" : \"result-result-container-6e907078\"})\n",
    "\n",
    "                infos = [job.a for job in jobs]\n",
    "                for info in infos:\n",
    "                    try:\n",
    "                        title = info.find(\"h2\").text.strip()\n",
    "                    except:\n",
    "                        title = \"\"\n",
    "                    try:\n",
    "                        company = info.find(\"div\", {\"class\" : \"result-result-subtitle-99125938\"}).text.strip().split(\",\")[0]\n",
    "                    except:\n",
    "                        company = \"\"\n",
    "                    try:\n",
    "                        city = info.find(\"div\", {\"class\" : \"result-result-subtitle-99125938\"}).text.strip().split(\",\")[1]\n",
    "                    except:\n",
    "                        city = \"\"\n",
    "                    try:\n",
    "                        time = info.time.text.strip()\n",
    "                    except:\n",
    "                        time = \"\"\n",
    "                    try:\n",
    "                        summary = info.find(\"div\", {\"class\" : \"result-result-description-c7581001\"}).text.strip()\n",
    "                    except:\n",
    "                        summary = \"\"\n",
    "                        \n",
    "                    source = \"XING\"\n",
    "\n",
    "                    job = {\n",
    "                                \"title\" : title,\n",
    "                                \"company\" : company,\n",
    "                                \"city\" : city,\n",
    "                                \"summary\" : summary,\n",
    "                                \"source\" : source\n",
    "                            }\n",
    "\n",
    "                    self.joblist.append(job)\n",
    "            \n",
    "            except:\n",
    "                print(\"no data found on page \" + str(page))\n",
    "        try:\n",
    "            df = pd.DataFrame(self.joblist)\n",
    "            return df\n",
    "        except:\n",
    "            return None\n",
    "        \n",
    "    def search_linkedin(self):\n",
    "        \n",
    "        print(\"Searching on LinkedIn...\")\n",
    "        \n",
    "        # list for the pages\n",
    "        linkedin_page_list = range(0, 100, 25)\n",
    "\n",
    "        linkedin_url = \"https://www.linkedin.com/jobs/search/?geoId=103035651&keywords={}&location={}&sortBy=DD&start={}\"\n",
    "        \n",
    "        # to modify url\n",
    "        linkedin_search = self.search.replace(\" \", \"%20\")\n",
    "        linkedin_place = self.place.replace(\" \", \"%20\")\n",
    "        \n",
    "        for page in linkedin_page_list:\n",
    "            try:\n",
    "                print(page)\n",
    "                headers = {\"User-Agent\" : random.choice(self.user_agent_list)}\n",
    "                r = requests.get(linkedin_url.format(linkedin_search, linkedin_place, page), headers = headers)\n",
    "                soup = bs(r.content, \"lxml\")\n",
    "                job_list = soup.find(\"section\", {\"class\" : \"results__list\"}).find(\"ul\")\n",
    "                print(len(job_list))\n",
    "\n",
    "                for job in job_list:\n",
    "                    try:\n",
    "                        title = job.a.span.text.strip()\n",
    "                    except:\n",
    "                        title = \"\"\n",
    "                    try:\n",
    "                        company = job.find(\"h4\").text.strip()\n",
    "                    except:\n",
    "                        company = \"\"\n",
    "                    try:\n",
    "                        city = job.find(\"span\", {\"class\" : \"job-result-card__location\"}).text.strip()\n",
    "                    except:\n",
    "                        city = \"\"\n",
    "\n",
    "                    summary = \"\"\n",
    "                    source = \"LinkedIn\"\n",
    "                    try:\n",
    "                        time = job.find(\"time\", {\"class\" : \"job-result-card__listdate--new\"}).text.strip()\n",
    "                    except:\n",
    "                        time = \"\"\n",
    "                    \n",
    "                    job = {\n",
    "                                \"title\" : title,\n",
    "                                \"company\" : company,\n",
    "                                \"city\" : city,\n",
    "                                \"summary\" : summary,\n",
    "                                \"source\" : source\n",
    "                            }\n",
    "\n",
    "                    self.joblist.append(job)\n",
    "                    \n",
    "            except:\n",
    "                print(\"no data found on page \" + str(page))\n",
    "        try:\n",
    "            df = pd.DataFrame(self.joblist)\n",
    "            return df\n",
    "        except:\n",
    "            return None\n",
    "                \n",
    "                \n",
    "                \n",
    "    def search_all(self):\n",
    "        \n",
    "        self.search_indeed()\n",
    "        self.search_steptstone()\n",
    "        self.search_stack_overflow()\n",
    "        self.search_xing()\n",
    "        self.search_linkedin()\n",
    "        \n",
    "        df = pd.DataFrame(self.joblist)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "\n",
    "jobs = JobFinder(search, place)\n",
    "df = jobs.search_all()\n",
    "#df.to_excel(f\"job_{search}_in_{place}.xlsx\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
